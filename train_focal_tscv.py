#!/usr/bin/env python3
"""
Focal Loss + æ™‚ç³»åˆ—CVçµ±åˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
æ ¹æœ¬çš„ãªæ”¹å–„ã‚’å®Ÿè£…
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
import keras

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.data_fetcher import DataFetcher
from src.feature_engineering import FeatureEngineering
from src.model import LSTMModel, TimeSeriesDataPreparator
from src.focal_loss import categorical_focal_loss, binary_focal_loss
from src.time_series_cv import TimeSeriesCrossValidator, MarketEnvironmentSplitter, analyze_data_splits

class FocalLossTrainer:
    """
    Focal Loss + æ™‚ç³»åˆ—CVçµ±åˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
    """
    
    def __init__(self, ticker: str, lookback: int = 60, test_size: float = 0.2):
        """
        Args:
            ticker: éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰
            lookback: éå»ä½•æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            test_size: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
        """
        self.ticker = ticker
        self.lookback = lookback
        self.test_size = test_size
        
        # ãƒ‡ãƒ¼ã‚¿
        self.data = None
        self.feature_columns = None
        
        # ãƒ¢ãƒ‡ãƒ«
        self.regression_model = None
        self.classification_model = None
        self.peak_bottom_model = None
        
        # æ™‚ç³»åˆ—CVï¼ˆã‚ˆã‚Šé©åˆ‡ãªè¨­å®šï¼‰
        self.tscv = TimeSeriesCrossValidator(n_splits=3, test_size=0.3)
        self.mes = MarketEnvironmentSplitter()
        
        # çµæœä¿å­˜
        self.results = {}
    
    def load_and_prepare_data(self, period: str = "10y"):
        """
        ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
        """
        print("=" * 80)
        print("Step 1: Loading and preparing data...")
        print("=" * 80)
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        fetcher = DataFetcher(self.ticker, period=period)
        self.data = fetcher.fetch_data()
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        engineer = FeatureEngineering(self.data)
        self.data = engineer.add_technical_indicators()
        self.data = engineer.detect_peaks_and_bottoms()
        self.data = engineer.create_target_labels()
        self.data = engineer.create_time_series_features()
        self.data = engineer.handle_missing_values()
        
        print(f"Data shape: {self.data.shape}")
        print(f"Columns: {len(self.data.columns)}")
        
        return self.data
    
    def select_features(self):
        """
        å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚’é¸æŠ
        """
        # åŸºæœ¬çš„ãªOHLCV
        base_features = ['open', 'high', 'low', 'close', 'volume']
        
        # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
        technical_features = [col for col in self.data.columns if any(
            indicator in col for indicator in [
                'sma', 'ema', 'macd', 'rsi', 'stoch', 'bb', 'atr', 'obv',
                'return', 'volatility', 'volume_sma', 'volume_ratio'
            ]
        )]
        
        # æ™‚ç³»åˆ—ç‰¹å¾´é‡
        time_features = ['day_of_week', 'day_of_month', 'week_of_year', 'month', 'quarter']
        
        # å…¨ç‰¹å¾´é‡
        self.feature_columns = base_features + technical_features + time_features
        
        # å­˜åœ¨ã—ãªã„åˆ—ã‚’é™¤å¤–
        self.feature_columns = [col for col in self.feature_columns if col in self.data.columns]
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚„ä¸è¦ãªåˆ—ã‚’é™¤å¤–
        exclude_columns = ['next_day_return', 'target_return', 'target_class', 'target_class_mapped',
                           'target_peak_bottom', 'is_peak', 'is_bottom', 'buy_signal']
        self.feature_columns = [col for col in self.feature_columns if col not in exclude_columns]
        
        print(f"\nSelected {len(self.feature_columns)} features for training")
        
        return self.feature_columns
    
    def calculate_class_weights(self, y_train: np.ndarray, num_classes: int) -> dict:
        """
        ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã‚’è¨ˆç®—ï¼ˆFocal Lossç”¨ï¼‰
        """
        from sklearn.utils.class_weight import compute_class_weight
        
        # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’0ã‹ã‚‰å§‹ã¾ã‚‹ã‚ˆã†ã«å¤‰æ›
        y_train_shifted = y_train - np.min(y_train)
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’è¨ˆç®—
        class_counts = np.bincount(y_train_shifted.astype(int))
        total_samples = len(y_train)
        
        # ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã‚’è¨ˆç®—ï¼ˆã‚ˆã‚Šæ¥µç«¯ã«ï¼‰
        class_weights = {}
        for i in range(num_classes):
            if i < len(class_counts) and class_counts[i] > 0:
                # ä¸å‡è¡¡åº¦ã«åŸºã¥ã„ã¦é‡ã¿ã‚’è¨ˆç®—
                imbalance_ratio = total_samples / class_counts[i]
                # ã‚ˆã‚Šæ¥µç«¯ãªé‡ã¿ä»˜ã‘ï¼ˆæœ€å¤§50å€ï¼‰
                weight = min(imbalance_ratio * 2, 50.0)
                class_weights[i] = weight
            else:
                class_weights[i] = 1.0
        
        print(f"Class weights: {class_weights}")
        return class_weights
    
    def train_with_focal_loss(self, X_train: np.ndarray, y_train: np.ndarray, 
                             X_val: np.ndarray, y_val: np.ndarray,
                             model_type: str, epochs: int = 100, batch_size: int = 32,
                             y_train_raw: np.ndarray = None):
        """
        Focal Lossã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        
        Args:
            model_type: 'regression', 'classification', 'peak_bottom'
        """
        print(f"\nTraining {model_type} model with Focal Loss...")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
        input_shape = (X_train.shape[1], X_train.shape[2])
        model = LSTMModel(input_shape, model_name=f"{model_type}_focal")
        
        if model_type == 'regression':
            # å›å¸°ãƒ¢ãƒ‡ãƒ«
            model.build_regression_model(lstm_units=[128, 64], dropout_rate=0.3)
            
            # å›å¸°ç”¨ã®Focal Lossï¼ˆMSEãƒ™ãƒ¼ã‚¹ï¼‰
            model.model.compile(
                optimizer='adam',
                loss='mse',
                metrics=['mae']
            )
            
        elif model_type == 'classification':
            # åˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆæ–¹å‘äºˆæ¸¬ï¼‰
            model.build_classification_model(num_classes=3, lstm_units=[128, 64], dropout_rate=0.3)
            
            # ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã‚’è¨ˆç®—ï¼ˆå…ƒã®ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ï¼‰
            if y_train_raw is not None:
                class_weights = self.calculate_class_weights(y_train_raw, 3)
                alpha = [class_weights.get(i, 1.0) for i in range(3)]
            else:
                class_weights = None
                alpha = None
            
            # é€šå¸¸ã®Cross Entropy Lossã‚’ä½¿ç”¨
            model.model.compile(
                optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            
        elif model_type == 'peak_bottom':
            # å¤©åº•æ¤œå‡ºãƒ¢ãƒ‡ãƒ«
            model.build_classification_model(num_classes=3, lstm_units=[128, 64], dropout_rate=0.3)
            
            # ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã‚’è¨ˆç®—ï¼ˆå…ƒã®ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ï¼‰
            if y_train_raw is not None:
                class_weights = self.calculate_class_weights(y_train_raw, 3)
                alpha = [class_weights.get(i, 1.0) for i in range(3)]
            else:
                class_weights = None
                alpha = None
            
            # é€šå¸¸ã®Cross Entropy Lossã‚’ä½¿ç”¨
            model.model.compile(
                optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
        
        # å­¦ç¿’
        callbacks = [
            keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),
            keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5),
            keras.callbacks.ModelCheckpoint(
                f'models/{model_type}_focal_best.h5',
                save_best_only=True,
                monitor='val_loss'
            )
        ]
        
        history = model.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # å­¦ç¿’å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        self.plot_training_history(history, model_type)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        model.save_model()
        
        return model, history
    
    def plot_training_history(self, history, model_type: str):
        """
        å­¦ç¿’å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        """
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss
        axes[0].plot(history.history['loss'], label='Training Loss')
        axes[0].plot(history.history['val_loss'], label='Validation Loss')
        axes[0].set_title(f'{model_type.title()} Model - Loss')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].grid(True)
        
        # Metrics
        if 'accuracy' in history.history:
            axes[1].plot(history.history['accuracy'], label='Training Accuracy')
            axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')
            axes[1].set_title(f'{model_type.title()} Model - Accuracy')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('Accuracy')
            axes[1].legend()
            axes[1].grid(True)
        elif 'mae' in history.history:
            axes[1].plot(history.history['mae'], label='Training MAE')
            axes[1].plot(history.history['val_mae'], label='Validation MAE')
            axes[1].set_title(f'{model_type.title()} Model - MAE')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('MAE')
            axes[1].legend()
            axes[1].grid(True)
        
        plt.tight_layout()
        plt.savefig(f'data/{model_type}_focal_history.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def train_with_time_series_cv(self, epochs: int = 100, batch_size: int = 32):
        """
        æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§å­¦ç¿’
        """
        print("=" * 80)
        print("Step 2: Training with Time Series Cross Validation...")
        print("=" * 80)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        required_cols = self.feature_columns + ['target_return', 'target_class', 'target_peak_bottom']
        df_clean = self.data[required_cols].dropna()
        
        print(f"Clean data shape: {df_clean.shape}")
        
        # æ™‚ç³»åˆ—CVã§åˆ†å‰²
        splits = list(self.tscv.split(df_clean))
        print(f"Generated {len(splits)} time series splits")
        
        # åˆ†å‰²ã‚’åˆ†æ
        analyze_data_splits(df_clean, splits, [f"TS-CV {i+1}" for i in range(len(splits))])
        
        # å„foldã§å­¦ç¿’ãƒ»è©•ä¾¡
        fold_results = []
        
        for fold_idx, (train_idx, val_idx) in enumerate(splits):
            print(f"\n{'='*60}")
            print(f"Fold {fold_idx + 1}/{len(splits)}")
            print(f"{'='*60}")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
            train_data = df_clean.iloc[train_idx]
            val_data = df_clean.iloc[val_idx]
            
            print(f"Train: {len(train_data)} samples")
            print(f"Validation: {len(val_data)} samples")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            preparator = TimeSeriesDataPreparator(lookback=self.lookback)
            
            # å›å¸°ãƒ‡ãƒ¼ã‚¿
            X_train_reg, _, y_train_reg, _ = preparator.prepare_data(
                train_data, self.feature_columns, 'target_return', test_size=0.0
            )
            X_val_reg, _, y_val_reg, _ = preparator.prepare_data(
                val_data, self.feature_columns, 'target_return', test_size=0.0
            )
            
            # åˆ†é¡ãƒ‡ãƒ¼ã‚¿ï¼ˆæ–¹å‘ï¼‰- one-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            X_train_class, _, y_train_class_raw, _ = preparator.prepare_data(
                train_data, self.feature_columns, 'target_class', test_size=0.0
            )
            X_val_class, _, y_val_class_raw, _ = preparator.prepare_data(
                val_data, self.feature_columns, 'target_class', test_size=0.0
            )
            
            # one-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›
            y_train_class = tf.keras.utils.to_categorical(y_train_class_raw, num_classes=3)
            y_val_class = tf.keras.utils.to_categorical(y_val_class_raw, num_classes=3)
            
            # å¤©åº•ãƒ‡ãƒ¼ã‚¿ - one-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            X_train_peak, _, y_train_peak_raw, _ = preparator.prepare_data(
                train_data, self.feature_columns, 'target_peak_bottom', test_size=0.0
            )
            X_val_peak, _, y_val_peak_raw, _ = preparator.prepare_data(
                val_data, self.feature_columns, 'target_peak_bottom', test_size=0.0
            )
            
            # one-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›
            y_train_peak = tf.keras.utils.to_categorical(y_train_peak_raw, num_classes=3)
            y_val_peak = tf.keras.utils.to_categorical(y_val_peak_raw, num_classes=3)
            
            print(f"Regression data: X_train{X_train_reg.shape}, y_train{y_train_reg.shape}")
            print(f"Classification data: X_train{X_train_class.shape}, y_train{y_train_class.shape}")
            print(f"Peak/Bottom data: X_train{X_train_peak.shape}, y_train{y_train_peak.shape}")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆæœ€åˆã®foldã®ã¿ï¼‰
            if fold_idx == 0:
                # å›å¸°ãƒ¢ãƒ‡ãƒ«
                self.regression_model, reg_history = self.train_with_focal_loss(
                    X_train_reg, y_train_reg, X_val_reg, y_val_reg,
                    'regression', epochs=epochs, batch_size=batch_size
                )
                
                # åˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆæ–¹å‘ï¼‰
                self.classification_model, class_history = self.train_with_focal_loss(
                    X_train_class, y_train_class, X_val_class, y_val_class,
                    'classification', epochs=epochs, batch_size=batch_size,
                    y_train_raw=y_train_class_raw
                )
                
                # å¤©åº•ãƒ¢ãƒ‡ãƒ«
                self.peak_bottom_model, peak_history = self.train_with_focal_loss(
                    X_train_peak, y_train_peak, X_val_peak, y_val_peak,
                    'peak_bottom', epochs=epochs, batch_size=batch_size,
                    y_train_raw=y_train_peak_raw
                )
            
            # è©•ä¾¡ï¼ˆå…¨foldï¼‰
            fold_result = self.evaluate_fold(
                fold_idx, X_val_reg, y_val_reg, X_val_class, y_val_class, 
                X_val_peak, y_val_peak
            )
            fold_results.append(fold_result)
        
        # å…¨foldã®çµæœã‚’ã¾ã¨ã‚ã‚‹
        self.summarize_cv_results(fold_results)
        
        return fold_results
    
    def evaluate_fold(self, fold_idx: int, X_val_reg: np.ndarray, y_val_reg: np.ndarray,
                     X_val_class: np.ndarray, y_val_class: np.ndarray,
                     X_val_peak: np.ndarray, y_val_peak: np.ndarray) -> dict:
        """
        å„foldã®è©•ä¾¡
        """
        if fold_idx == 0:  # æœ€åˆã®foldã®ã¿è©•ä¾¡ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’æ¸ˆã¿ã®å ´åˆï¼‰
            # äºˆæ¸¬
            y_pred_reg = self.regression_model.model.predict(X_val_reg, verbose=0)
            y_pred_class = self.classification_model.model.predict(X_val_class, verbose=0)
            y_pred_peak = self.peak_bottom_model.model.predict(X_val_peak, verbose=0)
            
            # åˆ†é¡äºˆæ¸¬ã‚’argmaxã«å¤‰æ›
            y_pred_class = np.argmax(y_pred_class, axis=1)
            y_pred_peak = np.argmax(y_pred_peak, axis=1)
            
            # çœŸã®å€¤ã‚‚one-hotã‹ã‚‰æ•´æ•°ãƒ©ãƒ™ãƒ«ã«å¤‰æ›
            y_true_class = np.argmax(y_val_class, axis=1)
            y_true_peak = np.argmax(y_val_peak, axis=1)
            
            # è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
            from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
            from sklearn.metrics import accuracy_score, classification_report
            
            reg_mae = mean_absolute_error(y_val_reg, y_pred_reg)
            reg_mse = mean_squared_error(y_val_reg, y_pred_reg)
            reg_r2 = r2_score(y_val_reg, y_pred_reg)
            
            class_acc = accuracy_score(y_true_class, y_pred_class)
            peak_acc = accuracy_score(y_true_peak, y_pred_peak)
            
            result = {
                'fold': fold_idx,
                'regression': {'mae': reg_mae, 'mse': reg_mse, 'r2': reg_r2},
                'classification': {'accuracy': class_acc},
                'peak_bottom': {'accuracy': peak_acc},
                'predictions': {
                    'regression': y_pred_reg.flatten(),
                    'classification': y_pred_class,
                    'peak_bottom': y_pred_peak
                }
            }
            
            print(f"Fold {fold_idx + 1} Results:")
            print(f"  Regression - MAE: {reg_mae:.4f}, MSE: {reg_mse:.4f}, R2: {reg_r2:.4f}")
            print(f"  Classification - Accuracy: {class_acc:.4f}")
            print(f"  Peak/Bottom - Accuracy: {peak_acc:.4f}")
            
            return result
        else:
            return {'fold': fold_idx, 'skipped': True}
    
    def summarize_cv_results(self, fold_results: list):
        """
        ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’ã¾ã¨ã‚ã‚‹
        """
        print("\n" + "=" * 80)
        print("Cross Validation Results Summary")
        print("=" * 80)
        
        # æœ‰åŠ¹ãªçµæœã®ã¿ã‚’æŠ½å‡º
        valid_results = [r for r in fold_results if 'skipped' not in r]
        
        if valid_results:
            # å›å¸°çµæœ
            reg_maes = [r['regression']['mae'] for r in valid_results]
            reg_mses = [r['regression']['mse'] for r in valid_results]
            reg_r2s = [r['regression']['r2'] for r in valid_results]
            
            print(f"\nRegression Results:")
            print(f"  MAE: {np.mean(reg_maes):.4f} Â± {np.std(reg_maes):.4f}")
            print(f"  MSE: {np.mean(reg_mses):.4f} Â± {np.std(reg_mses):.4f}")
            print(f"  R2: {np.mean(reg_r2s):.4f} Â± {np.std(reg_r2s):.4f}")
            
            # åˆ†é¡çµæœ
            class_accs = [r['classification']['accuracy'] for r in valid_results]
            peak_accs = [r['peak_bottom']['accuracy'] for r in valid_results]
            
            print(f"\nClassification Results:")
            print(f"  Direction Accuracy: {np.mean(class_accs):.4f} Â± {np.std(class_accs):.4f}")
            print(f"  Peak/Bottom Accuracy: {np.mean(peak_accs):.4f} Â± {np.std(peak_accs):.4f}")
        
        self.results['cv_results'] = fold_results
    
    def train_all_models_focal(self, epochs: int = 100, batch_size: int = 32):
        """
        Focal Loss + æ™‚ç³»åˆ—CVã§å…¨ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        """
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        self.load_and_prepare_data(period="10y")
        self.select_features()
        
        # æ™‚ç³»åˆ—CVã§å­¦ç¿’
        cv_results = self.train_with_time_series_cv(epochs=epochs, batch_size=batch_size)
        
        print("\n" + "=" * 80)
        print("Focal Loss + Time Series CV Training Completed!")
        print("=" * 80)
        
        return cv_results

def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("=" * 80)
    print("Focal Loss + æ™‚ç³»åˆ—CV çµ±åˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼")
    print("=" * 80)
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
    trainer = FocalLossTrainer("7203.T", lookback=60, test_size=0.2)
    
    # å­¦ç¿’å®Ÿè¡Œ
    results = trainer.train_all_models_focal(epochs=100, batch_size=32)
    
    print("\nğŸ‰ å­¦ç¿’å®Œäº†ï¼")
    print("ğŸ“Š çµæœ:")
    print("  - Focal Lossé©ç”¨")
    print("  - æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
    print("  - æ¥µç«¯ãªã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆ")
    print("  - ãƒ¢ãƒ‡ãƒ«: models/*_focal_best.h5")
    print("  - å±¥æ­´: data/*_focal_history.png")

if __name__ == "__main__":
    main()
